#Physical disks
In the example below there are four physical disks installed in the machine. On the supermicros we have multipathing enabled
It can bee seen below that there are eight logical disks. 2 paths to each. Some of the disks are partitioned, some are not.

````
root@manstra#fdisk -l|grep sd

Disk /dev/sdd: 300.0 GB, 300000000000 bytes
/dev/sdd1               1       36473   292967424   fd  Linux raid autodetect

Disk /dev/sdc: 300.0 GB, 300000000000 bytes
/dev/sdc1               1       36473   292967424   fd  Linux raid autodetect

Disk /dev/sda: 300.0 GB, 300000000000 bytes
/dev/sda1               1        2089    16777216   fd  Linux raid autodetect
/dev/sda2   *        2089        2121      256000   fd  Linux raid autodetect
/dev/sda3            2121       36473   275934208   fd  Linux raid autodetect

Disk /dev/sdb: 300.0 GB, 300000000000 bytes
/dev/sdb1               1        2089    16777216   fd  Linux raid autodetect
/dev/sdb2   *        2089        2121      256000   fd  Linux raid autodetect
/dev/sdb3            2121       36473   275934208   fd  Linux raid autodetect

Disk /dev/sde: 300.0 GB, 300000000000 bytes
/dev/sde1               1        2089    16777216   fd  Linux raid autodetect
/dev/sde2   *        2089        2121      256000   fd  Linux raid autodetect
/dev/sde3            2121       36473   275934208   fd  Linux raid autodetect

Disk /dev/sdf: 300.0 GB, 300000000000 bytes
/dev/sdf1               1        2089    16777216   fd  Linux raid autodetect
/dev/sdf2   *        2089        2121      256000   fd  Linux raid autodetect
/dev/sdf3            2121       36473   275934208   fd  Linux raid autodetect

Disk /dev/sdh: 300.0 GB, 300000000000 bytes
/dev/sdh1               1       36473   292967424   fd  Linux raid autodetect
Disk /dev/sdg: 300.0 GB, 300000000000 bytes
/dev/sdg1               1       36473   292967424   fd  Linux raid autodetect
````

#Multipathing
From the output below it can be seen that sdd and sdh are the multipath routes to the same disk. sdc and sdg are multipathed, etc.
````
root@manstra#multipath -l

mpathd (35000c50053f661fb) dm-7 SEAGATE,ST9300653SS
size=279G features='0' hwhandler='0' wp=rw
|-+- policy='round-robin 0' prio=0 status=active
| `- 0:0:3:0 sdd 8:48  active undef running
`-+- policy='round-robin 0' prio=0 status=enabled
  `- 0:0:8:0 sdh 8:112 active undef running

mpathc (35000c50053f6180b) dm-0 SEAGATE,ST9300653SS
size=279G features='0' hwhandler='0' wp=rw
|-+- policy='round-robin 0' prio=0 status=active
| `- 0:0:2:0 sdc 8:32  active undef running
`-+- policy='round-robin 0' prio=0 status=enabled
  `- 0:0:7:0 sdg 8:96  active undef running

mpathb (35000c50053f61d07) dm-1 SEAGATE,ST9300653SS
size=279G features='0' hwhandler='0' wp=rw
|-+- policy='round-robin 0' prio=0 status=active
| `- 0:0:1:0 sdb 8:16  active undef running
`-+- policy='round-robin 0' prio=0 status=enabled
  `- 0:0:6:0 sdf 8:80  active undef running

mpatha (35000c50053f666cb) dm-6 SEAGATE,ST9300653SS
size=279G features='0' hwhandler='0' wp=rw
|-+- policy='round-robin 0' prio=0 status=active
| `- 0:0:0:0 sda 8:0   active undef running
`-+- policy='round-robin 0' prio=0 status=enabled
  `- 0:0:5:0 sde 8:64  active undef running

```` 

#RAID devices
First create the partitions on the underlying physical disks with
````
#fdisk /dev/sdh (for no multipath disks)

#fdisk fdisk /dev/mapper/mpathe (for multipath disks)
````
Create the metadevice using mdadm
eg
````
# /sbin/mdadm --create --verbose /dev/md4 --level=1 --raid-devices=2 /dev/sde1 /dev/sdh1 (for non multipath disks)

/sbin/mdadm --create --verbose /dev/md3 --level=1 --raid-devices=2 /dev/mapper/mpathep1 /dev/mapper/mpathfp1 (for multipath disks)
````
Once created, the RAID device can be queried at any time to provide status information. The following example shows the output from the command
````
root@manstra#cat /proc/mdstst

root@manstra#mdadm --detail /dev/md4
/dev/md2:
        Version : 1.1
  Creation Time : Wed Dec 11 13:55:10 2013
     Raid Level : raid1
     Array Size : 275802944 (263.03 GiB 282.42 GB)
  Used Dev Size : 275802944 (263.03 GiB 282.42 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

  Intent Bitmap : Internal

    Update Time : Wed May  6 14:25:59 2015
          State : active
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : manstra:2
           UUID : e3196aa8:6fc57454:8552bfa5:8956fee4
         Events : 422

    Number   Major   Minor   RaidDevice State
       0     253       10        0      active sync   /dev/dm-10
       1     253        5        1      active sync   /dev/dm-5

````
The device numbers here dm-10 and dm-5 are generated by the mdadm creation and can be made sense of by listing the /dev/mapper directory



Config file
````
root@manstra#cat /etc/mdadm.conf
# mdadm.conf written out by anaconda
MAILADDR root
AUTO +imsm +1.x -all
ARRAY /dev/md0 level=raid1 num-devices=2 UUID=7d2e47b6:f66baed0:026875d5:b5722c81
ARRAY /dev/md1 level=raid1 num-devices=2 UUID=a9bc0e4a:7c636639:4310d804:aa6e74bf
ARRAY /dev/md2 level=raid1 num-devices=2 UUID=e3196aa8:6fc57454:8552bfa5:8956fee4
ARRAY /dev/md3 level=raid1 num-devices=2 UUID=bc814e8f:374ba3d3:bb984338:38c72ffc

root@manstra#ls -l /dev/mapper

total 0
crw-rw---- 1 root root 10, 58 May  6 13:53 control
lrwxrwxrwx 1 root root      7 May  6 13:53 mpatha -> ../dm-6
lrwxrwxrwx 1 root root      7 May  6 13:53 mpathap1 -> ../dm-8
lrwxrwxrwx 1 root root      7 May  6 13:53 mpathap2 -> ../dm-9
lrwxrwxrwx 1 root root      8 May  6 13:53 mpathap3 -> ../dm-10
lrwxrwxrwx 1 root root      7 May  6 13:53 mpathb -> ../dm-1
lrwxrwxrwx 1 root root      7 May  6 13:53 mpathbp1 -> ../dm-3
lrwxrwxrwx 1 root root      7 May  6 13:53 mpathbp2 -> ../dm-4
lrwxrwxrwx 1 root root      7 May  6 13:53 mpathbp3 -> ../dm-5
lrwxrwxrwx 1 root root      7 May  6 13:53 mpathc -> ../dm-0
lrwxrwxrwx 1 root root      7 May  6 13:53 mpathcp1 -> ../dm-2
lrwxrwxrwx 1 root root      7 May  6 13:53 mpathd -> ../dm-7
lrwxrwxrwx 1 root root      8 May  6 13:53 mpathdp1 -> ../dm-11
lrwxrwxrwx 1 root root      8 May  6 13:53 rootvg-easy -> ../dm-17
lrwxrwxrwx 1 root root      8 May  6 13:53 rootvg-root -> ../dm-12
lrwxrwxrwx 1 root root      8 May  6 13:53 rootvg-tmp -> ../dm-16
lrwxrwxrwx 1 root root      8 May  6 13:53 rootvg-var -> ../dm-15
````
It can be seen that
````
    dm-10 is on mpathap3 
    dm-5 is on mpathbp3.
````
In turn (from the multipath command)
````
    mpatha is sda and sde
    mpathb is sdb and sdf
````
In turn (from the fdisk command) sda p3 (partition3) is
````
    /dev/sda3            2121       36473   275934208   fd  Linux raid autodetect
sde p3 (partition3) is
    /dev/sde3            2121       36473   275934208   fd  Linux raid autodetect
These partitiions are two different paths to the same physical partition. They are mirrored to:
sdb p3 (partition3) is
    /dev/sdb3            2121       36473   275934208   fd  Linux raid autodetect
sdf p3 (partition3) is
    /dev/sdf3            2121       36473   275934208   fd  Linux raid autodetect
````

You can also use the command lsblk to make sense of things.

````
root@manstra#lsblk

NAME                        MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT

sdd                           8:48   0 279.4G  0 disk

¿¿mpathd (dm-7)             253:7    0 279.4G  0 mpath

  ¿¿mpathdp1 (dm-11)        253:11   0 279.4G  0 part

    ¿¿md3                     9:3    0 279.3G  0 raid1

sdc                           8:32   0 279.4G  0 disk

¿¿mpathc (dm-0)             253:0    0 279.4G  0 mpath

  ¿¿mpathcp1 (dm-2)         253:2    0 279.4G  0 part

    ¿¿md3                     9:3    0 279.3G  0 raid1

sda                           8:0    0 279.4G  0 disk

¿¿mpatha (dm-6)             253:6    0 279.4G  0 mpath

  ¿¿mpathap1 (dm-8)         253:8    0    16G  0 part

  ¿ ¿¿md1                     9:1    0    16G  0 raid1 [SWAP]

  ¿¿mpathap2 (dm-9)         253:9    0   250M  0 part

  ¿ ¿¿md0                     9:0    0   250M  0 raid1 /boot

  ¿¿mpathap3 (dm-10)        253:10   0 263.2G  0 part

    ¿¿md2                     9:2    0   263G  0 raid1

      ¿¿rootvg-root (dm-12) 253:12   0    20G  0 lvm   /

      ¿¿rootvg-var (dm-15)  253:15   0    10G  0 lvm   /var

      ¿¿rootvg-tmp (dm-16)  253:16   0     8G  0 lvm   /tmp

      ¿¿rootvg-easy (dm-17) 253:17   0     4G  0 lvm   /easy

sdb                           8:16   0 279.4G  0 disk

¿¿mpathb (dm-1)             253:1    0 279.4G  0 mpath

  ¿¿mpathbp1 (dm-3)         253:3    0    16G  0 part

  ¿ ¿¿md1                     9:1    0    16G  0 raid1 [SWAP]

  ¿¿mpathbp2 (dm-4)         253:4    0   250M  0 part

  ¿ ¿¿md0                     9:0    0   250M  0 raid1 /boot

  ¿¿mpathbp3 (dm-5)         253:5    0 263.2G  0 part

    ¿¿md2                     9:2    0   263G  0 raid1

      ¿¿rootvg-root (dm-12) 253:12   0    20G  0 lvm   /

      ¿¿rootvg-var (dm-15)  253:15   0    10G  0 lvm   /var

      ¿¿rootvg-tmp (dm-16)  253:16   0     8G  0 lvm   /tmp

      ¿¿rootvg-easy (dm-17) 253:17   0     4G  0 lvm   /easy

sde                           8:64   0 279.4G  0 disk

¿¿mpatha (dm-6)             253:6    0 279.4G  0 mpath

  ¿¿mpathap1 (dm-8)         253:8    0    16G  0 part

  ¿ ¿¿md1                     9:1    0    16G  0 raid1 [SWAP]

  ¿¿mpathap2 (dm-9)         253:9    0   250M  0 part

  ¿ ¿¿md0                     9:0    0   250M  0 raid1 /boot

  ¿¿mpathap3 (dm-10)        253:10   0 263.2G  0 part

    ¿¿md2                     9:2    0   263G  0 raid1

      ¿¿rootvg-root (dm-12) 253:12   0    20G  0 lvm   /

      ¿¿rootvg-var (dm-15)  253:15   0    10G  0 lvm   /var

      ¿¿rootvg-tmp (dm-16)  253:16   0     8G  0 lvm   /tmp

      ¿¿rootvg-easy (dm-17) 253:17   0     4G  0 lvm   /easy

sdf                           8:80   0 279.4G  0 disk

¿¿mpathb (dm-1)             253:1    0 279.4G  0 mpath

  ¿¿mpathbp1 (dm-3)         253:3    0    16G  0 part

  ¿ ¿¿md1                     9:1    0    16G  0 raid1 [SWAP]

  ¿¿mpathbp2 (dm-4)         253:4    0   250M  0 part

  ¿ ¿¿md0                     9:0    0   250M  0 raid1 /boot

  ¿¿mpathbp3 (dm-5)         253:5    0 263.2G  0 part

    ¿¿md2                     9:2    0   263G  0 raid1

      ¿¿rootvg-root (dm-12) 253:12   0    20G  0 lvm   /

      ¿¿rootvg-var (dm-15)  253:15   0    10G  0 lvm   /var

      ¿¿rootvg-tmp (dm-16)  253:16   0     8G  0 lvm   /tmp

      ¿¿rootvg-easy (dm-17) 253:17   0     4G  0 lvm   /easy

sdh                           8:112  0 279.4G  0 disk

¿¿mpathd (dm-7)             253:7    0 279.4G  0 mpath

  ¿¿mpathdp1 (dm-11)        253:11   0 279.4G  0 part

    ¿¿md3                     9:3    0 279.3G  0 raid1

sdg                           8:96   0 279.4G  0 disk

¿¿mpathc (dm-0)             253:0    0 279.4G  0 mpath

  ¿¿mpathcp1 (dm-2)         253:2    0 279.4G  0 part

    ¿¿md3                     9:3    0 279.3G  0 raid1

root@manstra#
````
 

##REFERENCE
LVM - Logical Volume Manager;
RAID - Redundant Array of Independent (old: Inexpensive) Disks;

There is no strong correlation between LVM and RAID, i.e., each technology can be used independently. However, striped LVM usually requires RAID1. In the following examples, LVM means in fact LVM2, and RAID is assumed to be Linux Software RAID. Hardware RAID is less complicated, but (!) do not mix real (heavy | expensive | server) hardware RAID with pseudo (fake) RAID integrated on typical PC motherboard. In last case Linux Software RAID is preferred.

RAID + Striped LVM
To create LVM with striping, at least 4 disks are required. First, we create partitionsLinux raid autodetect(1 disk = 1 partition). Then, assuming disks aresd[c-f], two RAID1 are created:

````
mdadm –-create /dev/md0 –-level=raid1
–-raid-devices=2 /dev/sd[cd]1

mdadm –-create /dev/md1 –-level=raid1
–-raid-devices=2 /dev/sd[ef]1
````

RAID config is kept in/etc/mdadm.conf. However, mdadm can handle array without this config file. Now, we create physical volumes:

````
pvcreate /dev/md0

pvcreate /dev/md1

````
In case of hardware RAID, mdadm is not used, and physical volumes are created as shown below:

````
pvcreate /dev/sdb

pvcreate /dev/sdc

````
After this, we create a volume group (name can be selected):

````
vgcreate VolGroup01 /dev/md0 /dev/md1

````
or (for HW RAID)

````
vgcreate VolGroup01 /dev/sdb /dev/sdc

````
The following commands create two logical volumes distributed accross two arrays, the stripe size is256K, the volume sizes are136GB:

````
lvcreate -i2 -I256 -L136G -nLogVol00 VolGroup01

lvcreate -i2 -I256 -L136G -nLogVol01 VolGroup01

````
And finally, file systems are created

````
mke2fs -c -j -T news /dev/VolGroup01/LogVol00

mke2fs -c -j -T news /dev/VolGroup01/LogVol01

````
and mounted like follows:

````
mount /dev/VolGroup01/LogVol00 /u02

mount /dev/VolGroup01/LogVol01 /u03

````
Other useful commands are:

````
pvdisplay   displays attributes of a physical volume;

vgdisplay   displays attributes of volume group;

lvdisplay   displays attributes of a logical volume;

pvscan   scans all disks for physical volumes;

vgscan   scans all disks for volume groups and rebuilds caches;

lvscan   scans all disks for logical volumes;

lvremove /dev/VolGroup01/LogVol01   removes LV;

vgchange -a n VolGroup01   deactivates a volume group;

vgremove VolGroup01   removes a volume group;

````
LVM without RAID (single disk)
There is no place here to discuss all advantages of LVM on a system with one or two disks. During Linux installation you may be prompted to use LVM, and if you choose, all config is done by setup program. The problems may arise when disk fails and you must manually recreate the whole structure (partitions, groups, volumes, etc). Let's assume that we're going to restore a system disk (filesystem dumps are available). First we create primary Linux partition for/bootand Linuxswappartition. The rest goes to PV:

````
pvcreate /dev/sda3

````
After this we create volume group and logical volumes:

````
vgcreate VolGroup00 /dev/sda3

lvcreate -L16G –-LogVol00 VolGroup00

lvcreate -L8G –-LogVol01 VolGroup00

lvcreate -L64G –-LogVol02 VolGroup00

````

The next stage:
````
mke2fs -c -j -T news /dev/VolGroup00/LogVol00
````

##How to access the striped volumes
when the system is broken
Let's assume, that the system [disk] [with all config files] has gone, but Software RAID array with striped LVM contains some usefull data. We can reassemble it in two steps: 1) RAID; 2) LVM. With hardware RAID there'll be the last step only.

To perform this task, we must load Linux OS using some appropriate Restore / Repair / Live CD supportingmultipath devicesandLVM2. First of all, you must re-createmdadm.conf:
````
echo "DEVICE partitions" > /etc/mdadm.conf

echo "MAILADDR root" >> /etc/mdadm.conf

mdadm –-examine –-scan /dev/sdc1 /dev/sdd1
/dev/sde1 /dev/sdf1 >> /etc/mdadm.conf
````
Then, try to assemble RAID:
````
mdadm –A -s

cat /proc/mdstat
````
The last command is optional (just to be sure). If RAID is O.K., the following cmds should be executed in the specified sequence:
````
vgscan

pvscan

vgchange -a y

lvscan
````
The last cmd must display that all logical volumes areACTIVE. Now volume can be mounted on some empty directory:
````
mount -o ro /dev/VolGroup01/LogVol00 /mnt
````
